% !TEX root =  ../Dissertation.tex

\chapter{Background}

\section{Reinforcement Learning}

Reinforcement Learning (RL) \cite{sutton2018reinforcement} is an artificial intelligence learning paradigm, which, along with supervised and unsupervised learning, constitutes one of the three main approaches in machine learning. What distinguishes Reinforcement Learning from other approaches is that the learning occurs through a trial-and-error process, where an \textbf{agent} receives \textbf{reward} signals (positive or negative) from an \textbf{environment}, leading to a focus on maximizing long-term positive rewards. The key elements, the agent and the environment, interact iteratively: the agent observes the state of the environment, takes an action accordingly, and receives a reward along with the next state from the environment (see Figure \ref{}). The primary goal of an RL agent is to maximize the accumulated reward, also known as the \textbf{return}. In this section, we will formally define these concepts.

% MDPs
\subsection{Markov Decision Process (MDP)}

Markov Decision Processes (MDP) provide the formal framework upon which most reinforcement learning algorithms are built. A \textbf{finite} MDP is an state transition system defined as a 5-tuple $\mathcal{M} = \langle \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma \rangle$, where 

\begin{itemize}
    \item $\mathcal{S}$ is a finite set of states,
    \item $\mathcal{A}$ is a finite set of actions,
    \item $P : S \times A \rightarrow \mathcal{P}(S)$ is a transition kernel, where $\mathcal{P}(S)$ is the set of probability distributions on $S$, and $P(s'|s, a)$ is the probability of transitioning from state $s$ to state $s'$,
    \item $\mathcal{R} : S \times A \rightarrow R$ is the reward function,
    \item and $\gamma \in [0, 1)$ is a discount factor.
\end{itemize}

For notational convenience, we define $P^a_s \in \mathcal{P}(S)$ to represent the next-state distribution for a given state-action pair $(s, a)$, and $r^a_s$ to denote the associated immediate reward. Note not get confused with $P^a_s \neq P(s'|s, a)$, the former represents a distribution and the second one a probability value.

For reference, the MDP can be visualized as a graph, as shown in Figure \ref{}, where the interaction of all these components is depicted. Intuitively, the reinforcement learning (RL) loop illustrated in Figure \ref{} can be understood as a trial-and-error process that induces an MDP. For example, if the states are represented by image frames from a simulator, our MDP will be described by \ref{}.

The previous notion defines what intuitively could be understand as the environment, while the agent decision-making mechanism is defined by a policy $\pi$. A \textbf{policy} is a mapping from states to distributions over actions (or just actions for discrete policies) $\pi \in \mathcal{P}(A)^x$.

\subsection{Trajectories, Return, and Value Functions}



% \subsection{Value Iteration}
% \subsection{Q-Learning}

\subsection{Deep Q-Learning (DQN)}
\subsection{Prioritized Experience Replay}

\section{Bisimulation}

\subsection{Bisimulation in a Markov Decision Process}
Initially introduced in the field of concurrency theory, \textbf{bisimulation} \cite{li2006towards, abate2024bisimulation, baier2008principles} is an equivalence relation between states of a transition system (e.g. MDP) that preserves the branching structure of the system, and which thus can simulate each other in a stepwise manner. Two states are bisimilar if they can simulate each other's behavior, thereby a bisimulation serves as a form of state abstraction that groups states \(s_i\) and \(s_j\) that are 'behaviorally equivalent'. In fact, their optimal value functions are equal, \(V^\ast(s_i) = V^\ast(s_j)\)

\textbf{Definition 1.} (Givan et al. \cite{givan2003equivalence}). Given an MDP $\mathcal{M}$, an equivalence relation $B$ between states is a \textbf{bisimulation relation} if, for all states $s_i, s_j \in \mathcal{S}$ that are equivalent under $B$ the following conditions hold:
% \begin{enumerate}
%     \item $\mathcal{R}\left(s_i, a\right) =\mathcal{R}\left(s_j, a\right) \quad \forall a \in \mathcal{A},$
%     \item $\mathcal{P}\left(G \mid s_i, a\right) =\mathcal{P}\left(G \mid s_j, a\right) \quad \forall a \in \mathcal{A}, \quad \forall G \in \mathcal{S}_B,$
% \end{enumerate}
\begin{equation}
\begin{aligned}
\mathcal{R}\left(s_i, a\right) & =\mathcal{R}\left(s_j, a\right) & & \forall a \in \mathcal{A}, \\
\mathcal{P}\left(C \mid s_i, a\right) & =\mathcal{P}\left(C \mid s_j, a\right) & & \forall a \in \mathcal{A}, \quad \forall C \in \mathcal{S}_B,
\end{aligned}
\end{equation}

where $\mathcal{S}_B$ is the partition of $\mathcal{S}$ under the relation $B$, and $\mathcal{P}(C|s, a) = \sum_{s' \in C} \mathcal{P}(s'|s, a)$.

%According to this concept, 
Two states $s_i, s_j \in S$ are \textbf{bisimilar} if there exists a bisimulation relation $B$ such that $(s_i, s_j) \in B$; consequently, their optimal value functions are equal, \(V^\ast(s_i) = V^\ast(s_j)\).




That is, any two states with distance 0 will be collapsed onto the same equivalence class.

\subsubsection{Equivalence Relation}

\begin{definition}[Equivalence Relation]
Let $X$ be a set. An equivalence relation on $X$ is a subset $R \subseteq X \times X$ that satisfies the following three properties:
\begin{enumerate}
    \item \textbf{Reflexivity}: For all $x \in X$, $(x,x) \in R$;
    \item \textbf{Symmetry}: For all $x, y \in X$, if $(x,y) \in R$ then $(y,x) \in R$;
    \item \textbf{Transitivity}: For all $x, y, z \in X$, if $(x,y) \in R$ and $(y,z) \in R$ then $(x,z) \in R$.
\end{enumerate}
\end{definition}

In mathematics, an equivalence relation is a binary relation that is reflexive, symmetric and transitive. A simpler example is equality. 

\begin{enumerate}
    \item Any number $a$ is equal to itself (reflexive).
    \item  If $a=b$, then $b=a$ (symmetric).
    \item If $a=b$ and $b=c$, then $a=c$ (transitive). 
\end{enumerate}

\subsection{Bisimulation metric}

\subsection{On-policy bisimulation}

\textbf{Definition 2.} (Castro \cite{castro2020scalable}). Given an MDP $\mathcal{M}$, an equivalence relation $B^\pi$ between states is a $\pi$-\textbf{bisimulation relation} if, for all states $s_i, s_j \in \mathcal{S}$ that are equivalent under $B^\pi$ the following conditions hold:
% \begin{enumerate}
%     \item $\mathcal{R}\left(s_i, a\right) =\mathcal{R}\left(s_j, a\right) \quad \forall a \in \mathcal{A},$
%     \item $\mathcal{P}\left(G \mid s_i, a\right) =\mathcal{P}\left(G \mid s_j, a\right) \quad \forall a \in \mathcal{A}, \quad \forall G \in \mathcal{S}_B,$
% \end{enumerate}

%\mathcal{R}\left(s_i, a\right) & =\mathcal{R}\left(s_j, a\right) & & \forall a \in \mathcal{A},

\begin{equation}
\begin{aligned}
\mathcal{R}_{s_i}^\pi & = \mathcal{R}_{s_j}^\pi
 \\
\mathcal{P}_{s_i}^\pi\left(C \right) & =\mathcal{P}_{s_j}^\pi\left(C\right) \quad \forall C \in \mathcal{S}_B,
\end{aligned}
\end{equation}

where $\mathcal{S}_{B^\pi}$ is the partition of $\mathcal{S}$ under the relation $B^\pi$, and

\begin{equation}
\begin{aligned}
\mathcal{R}_s^\pi & :=\sum_a \pi(a \mid s) \mathcal{R}(s, a) \\
\forall C \in \mathcal{S}_{B^\pi}, \mathcal{P}_s^\pi(C) & :=\sum_a \pi(a \mid s) \sum_{s^{\prime} \in C} P( s^{\prime} \mid s, a)
\end{aligned}
\end{equation}

%According to this concept, 
Two states $s_i, s_j \in S$ are $\pi$-\textbf{bisimilar} if there exists a $\pi$-bisimulation relation $B^\pi$ such that $(s_i, s_j) \in B^\pi$.

% On-policy Bisimulation metric

\subsection{On-policy Bisimulation metric}

Extending the work of Desharnais et al. (1999) for labeled Markov processes, Ferns, Panangaden, and Precup (2004) generalized the notion of MDP bisimulation relations to metrics, yielding a smoother notion of similarity than equivalence relations. 


Let $\mathcal{M(S)}= \{d \in [0, \infty)^{\mathcal{S} \times \mathcal{S}} : d \text{ symmetric and satisfies the triangle inequality}\}$ be the set of all pseudometrics on $S$. A pseudometric $d \in \mathcal{M}$ induces an equivalence relation $E_d := \{(s, t)|d(s, t) = 0\}$. \newline

\textbf{Definition 3.} (Castro \cite{castro2020scalable} Theorem 2) A $\pi$-bisimulation metric $d^\sim$ is the unique fixed-point of the operator $T^\pi : \mathcal{M(S)} \rightarrow \mathcal{M(S)}$, where 
\begin{equation}
    T^\pi_k(d)(s_i, s_j) = |\mathcal{R}^\pi_{s_i} - \mathcal{R}^\pi_{s_j}| + \gamma \mathcal{W}_d(\mathcal{P}^\pi_{s_i},\mathcal{P}^\pi_{s_j}) 
\end{equation}

where $\mathcal{W}_d$ corresponds to the Kantorovich distance (also known as Wasserstein distance) over the set of distributions $\mathcal{P}(\mathcal{S})$ with based distance $d$, defined as

\begin{equation}
W_d\left(\mu, \mu^{\prime}\right)=\min _{\substack{\left(Z, Z^{\prime}\right) \\ Z \sim \mu, Z^{\prime} \sim \nu^{\prime}}} \mathbb{E}\left[d\left(Z, Z^{\prime}\right)\right]
\end{equation}

where $u, u'\in \mathcal{P}(\mathcal{S})$, and $(Z,Z')$ denotes all the possible couplings.

The operator $T^\pi_k(d)$ works as standard operators in dynamic programming (e.g. value iteration), which will eventually converge to a fixed point $d^\sim$ up to an accuracy $\delta$, where $T^\pi_K(d)$ maps effectively $\mathcal{M}(S)$ into itself, and $T^\pi_K(d) = d^\sim : S \times S \rightarrow \mathbb{R}$. Then, let an initial estimate $d_0$

$$d_0 \rightarrow T^\pi_1(d_0) = d_1 \rightarrow T^\pi_2(d_1) = d_2 \cdots \rightarrow d^\sim$$

\begin{equation}
    T^\pi_k(d)(s_i, s_j) = |\mathcal{R}^\pi_{s_i} - \mathcal{R}^\pi_{s_j}| + \gamma d(\mathcal{N}(s_i,\pi(s_i),\mathcal{N}(s_j,\pi(s_j))) 
\end{equation}

where $\mathcal{N}$ corresponds to the next state, which in a deterministic has $P(\mathcal{N}(s,\pi(S)) = 1$, and $\pi(s)$ is the action taken by the current policy.

\subsection{Independent Couplings}

\section{Matching under Independent Couplings (MICO) metric}

\section{Diffuse metric}

%\subsection{State Abstractions}











