% !TEX root =  ../Dissertation.tex

\chapter{Literature Review}

\section{Non-Uniform Sampling Experience Replays}

PER \cite{schaul2015prioritized} is by far the most relevant non-uniformly sampling method, which samples visited experiences proportional to the absolute TD errors efficiently reducing converges time. Nonetheless, after these promising results, multiple attempts to improve and control even more the non-uniform sampling has been done to address different challenges such as: sparsing rewards assignation \cite{andrychowicz2017hindsight}, ..., ....,. This large variety of option makes complex and unclear to discern between different and complementary methods. In such way, the following proposed categorization looks to capture the main nuances of the variety of methodologies, and provide a guidelines to explore experiences replay methods, which exists currently in the literature. The categorization is as follows: Priority Substitution, Priority Reweighing, and Auxiliary Learnable Mechanisms.

\subsection{Priority Substitution}

 Even though PER is considered a state-of-the-art method and is widely used in various reinforcement learning algorithms as a mechanism to alleviate long training loops, its efficiency declines in challenging scenarios with sparse rewards, where the reward signal only appears after long trajectories. To address this issue, Andrychowicz et al. \cite{andrychowicz2017hindsight} proposed a hindsight method using universal policies, known as \textbf{Hindsight Experience Replay (HER)}. This method accepts both a current state and a goal state, encouraging the experience replay to sample experiences in hindsight; in other words, using a different goal on the fly than the one the agent was originally trying to achieve in the episode. HER effectively handles sparse and binary rewards without requiring additional reward manipulation. Our method, in contrast, does not require defining different goals on the fly. Instead, by learning a bisimulation metric dynamically (considering both immediate rewards and transitions), it approximates the behavioral similarity of theoretically all states in the environment, thereby addressing the problem of sparse rewards without the need for extra reward manipulation or goal-based policies.

Subsequent efforts have studied the retention and sampling capabilities of experience replay simultaneously; focusing on how many experiences to maintain in and sample from the experience replay.  Particularly, de Bruim et al. \cite{de2018experience} has explored a concept of \textbf{experience selection}, which address both retention and sampling by using different proxies to define priorities based on the immediate and long-term utility. However, they only provided implementation guidelines because the proxies can be highly sensible to the task at hand. In contrast, our method is based on an strong mathematical formalism (bisimulation \cite{li2006towards}), applicable to any discrete or stochastic environment, without the need for hand-engineering proxies.

Alternatively, Dopamine Rainbow \cite{hessel2018rainbow} evaluates the performance of six extensions of DQN algorithms, empirically demonstrating the significant importance of prioritized replay and \textbf{multi-step targets} in enhancing DQN performance. Unlike single-step targets in temporal difference (TD) error, a multi-step target considers n steps with intermediate actions determined by the behavioral policy. This approach is effectively used to substitute single-step td-errors priorities for multi-step td-error priorities to train DQN algorithms. The multi-step bootstrap target \cite{sutton1988learning, sutton2018reinforcement} efficiently adjusts the bias-variance trade-off, enabling the rapid propagation of newly observed rewards to previously visited states. Fedus et al. \cite{fedus2020revisiting} rigorously studied the effect of various components of experience replay on the DQN algorithm, revealing that multi-step targets are crucial for leveraging replay buffers with large capacities, despite the significant degree of off-policyness they may introduce. Our method, BPER, controls the influence on priority by weighting the use of single-step temporal difference (TD) error and the bisimulation metric with a hyperparameter. Although our method could potentially overlook TD error in extreme cases where only the bisimulation metric is selected (maximum weight equal 1), single-step targets are still necessary for optimizing the main reinforcement learning objective. Our approach does not impose any constraints on the use of n-step targets for both Q-learning and priority, and they can be included to enhance the performance of a DQN algorithm. Instead, our method functions as a reweighting mechanism between TD errors and the bisimulation metric.

% Me quede aqui
% Recuerda en el paper de PER hay algunas citas que podrian ser utiles solo usa una mas


Density-based Prioritization: This method focuses on maintaining a diverse set of experiences in the replay buffer by ensuring that experiences are sampled from low-density regions of the state space. This helps in covering the state space more uniformly.
Likelihood-free Importance Weighting: Sinha et al. (2022) introduced a method that reweights priorities by estimating the likelihood-free density ratio between on-policy and off-policy experiences.

% I'm here

\subsection{Priority Reweighing}

Finally, we analyze methods that aim to reweigh priorities more efficiently. Kumar et al. \cite{kumar2020discor}  propose a corrective feedback method, reweighing samples by adjusting the TD-error target value with an estimated target value error, mitigating error accumulation. Sun et al. \cite{sun2020attentive} propose Attention Experience Replay (AER), implicitly assigning high priorities to transitions containing frequently visited states. Liu et al. \cite{liu2021regret} propose an optimal prioritization strategy based on regret minimization, indicating transitions with higher hindsight TD error should be prioritized. This work implements two methods: ReMERN, learning an error network to assign priorities, and ReMERT, exploiting temporal state ordering without an additional neural network. Concurrently, SUNRISE \cite{lee2021sunrise} reweighs target Q-values based on uncertainty estimates from a Q-ensemble, improving signal-to-noise in Q-updates and stabilizing learning. Lastly, Sinha et al. \cite{sinha2022experience}  propose a reweighing technique, reassigning priorities by the likelihood-free density ratio between on-policy and off-policy experiences.


\subsection{Auxiliary Mechanism}
% \subsection{Model-based Sampling}

Additional methods focus on incorporating auxiliary mechanisms to reduce dependence on priority during sampling. Zha et al. \cite{zha2019experience} include an additional replay policy learned to filter irrelevant experiences and use priorities only on relevant ones. Similarly, Pan et al. \cite{pan2022understanding} propose a model-based Stochastic Gradient Langevin Dynamics (SGLD) sampling method, producing hypothetical experiences using the current policy and concatenating them with uniformly sampled experiences from an ER. SGLD addresses outdated priorities and insufficient sample space coverage. In contrast, by learning abstractions, our method does not require an additional replay policy learning, complex world model, or hypothetical state tracking. Instead, our method reduces training complexity by learning lower-dimensional state representations, which are organized by behavioral quality in a latent space.


Our work aligns with methods that reweigh priorities but also includes an auxiliary mechanism to calculate bisimulation metrics. Specifically, our method implicitly learns a bisimulation (and bisimulation metric) by learning state abstractions with an encoder neural network. This network is updated with an objective loss function, which encourages to keep behaviorally similar states closer together and behaviorally dissimilar states farther apart in a latent space. The bisimulation metric defines a behavioral similarity distance, which will be used to reweigh priorities, encouraging more diverse sampling.

\section{Bisimulation Metrics}

% An explanation of the overview of bisimulation metric, but the detailed understanding of what it needed to this thesis is in background, which was written before so mentioning this as other examples 

\subsection{Dynamical Programming Methods}
\subsection{Deep Learning Methods}
\subsection{Kernel Methods}
\section{Learning State Abstractions}
\subsection{Reconstructions Abstractions}
\subsection{Contrastive Abstractions}
\subsection{Behavioral Abstractions}