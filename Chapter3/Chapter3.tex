% !TEX root =  ../Dissertation.tex

\chapter{Literature Review}

\section{Non-Uniform Sampling Experience Replays}

Prioritized Experience Replay (PER) \cite{schaul2015prioritized} is by far the most relevant non-uniform sampling method, which samples visited experiences proportional to the absolute TD errors, thereby efficiently reducing convergence time. However, following these promising results, numerous attempts have been made to further improve and refine non-uniform sampling to address various challenges, such as sparse reward assignment \cite{andrychowicz2017hindsight, dai2021diversity}, experience retention \cite{de2018experience}, the bias-variance trade-off \cite{fedus2020revisiting, hessel2018rainbow, sutton1988learning, sutton2018reinforcement}, and trajectory sampling \cite{dai2021diversity, liu2023prioritized}. The wide variety of approaches can make it complex and unclear to differentiate between distinct and complementary methods. To clarify this landscape, the following proposed categorization aims to capture the main nuances of these methodologies and provide guidelines for exploring the current experience replay methods in the literature. The categorization is as follows: Priority Substitution, Priority Reweighting, and Auxiliary Learnable Mechanisms.

\subsection{Priority Substitution}

 Even though PER is considered a state-of-the-art method and is widely used in various reinforcement learning algorithms as a mechanism to alleviate long training loops, its efficiency declines in challenging scenarios, e.g: with sparse rewards, where the reward signal only appears after long trajectories. To address this issue, Andrychowicz et al. \cite{andrychowicz2017hindsight} proposed a hindsight method using universal policies, known as \textbf{Hindsight Experience Replay (HER)}. This method accepts both a current state and a goal state, encouraging the experience replay to sample experiences in hindsight; in other words, using a different goal on the fly than the one the agent was originally trying to achieve in the episode. HER effectively handles sparse and binary rewards without requiring additional reward manipulation. Our method, in contrast, does not require defining different goals on the fly. Instead, by learning a bisimulation metric dynamically (considering both immediate rewards and transitions), it approximates the behavioral similarity of theoretically all states in the environment, thereby dealing with sparse rewards without the need for extra reward manipulation or goal-based policies.

Subsequent efforts have studied the retention and sampling capabilities of experience replay simultaneously; focusing on how many experiences to maintain in and sample from the experience replay.  Particularly, de Bruim et al. \cite{de2018experience} has explored a concept of \textbf{experience selection}, which address both retention and sampling by using different proxies to define priorities based on the immediate and long-term utility. However, they only provided implementation guidelines because the proxies can be highly sensible to the task at hand. In contrast, our method is based on an strong mathematical formalism (bisimulation \cite{li2006towards}), applicable to any discrete or stochastic environment, and able to adjust to any task at hand, without the need for hand-engineering proxies.

Alternatively, Dopamine Rainbow \cite{hessel2018rainbow} empirically evaluates six alternatives of DQN algorithms, demonstrating the significant importance of prioritized replay and \textbf{multi-step targets} in enhancing DQN performance. Unlike single-step targets in temporal difference (TD) error, a multi-step target considers n steps with intermediate actions determined by the behavioral policy. This approach effectively substitutes single-step td-errors priorities for multi-step td-error priorities to train DQN algorithms. The multi-step bootstrap target \cite{sutton1988learning, sutton2018reinforcement} efficiently balances the bias-variance trade-off, enabling the rapid propagation of newly observed rewards to previously visited states. Consequently, Fedus et al. \cite{fedus2020revisiting} rigorously studied the effect of various components of experience replay on the DQN algorithm, revealing that multi-step targets are crucial for leveraging replay buffers with large capacities, despite the significant degree of off-policyness they may introduce. Our method, BPER, assigns priority by weighting a single-step TD error and the bisimulation metric with a hyperparameter. Although our method could potentially overlook TD error in extreme cases where only the bisimulation metric is used for priority (i.e., when the maximum weight is set to 1), single-step targets are still necessary for optimizing the main reinforcement learning objective. Our approach does not impose any constraints on the use of n-step targets for both Q-learning and priority, and they can be incorporated to further enhance the performance of a DQN algorithm.

Recent works have explored the use of \textbf{trajectories} for both experience replay and priority assignment. Dai et al. \cite{dai2021diversity} proposed a two-stage method to enhance Hindsight Experience Replay (HER) by incorporating diversity-based trajectory sampling. First, trajectories are non-uniformly sampled from an experience replay buffer using priorities based on determinantal point processes (DPPs). Second, a k-DPP is applied to each trajectory to sample transitions with diverse goal states from the previously selected trajectories during training. Without relying on semantic knowledge of the goal space or tuning a curriculum, this method achieves efficient training and performance in robotic manipulation tasks. Subsequently, Liu et al. \cite{liu2023prioritized} further explored the concept of Trajectory Replay (TR), which stores complete trajectories instead of individual transition tuples. In these methods, transitions are sampled in a backward manner, building on the findings of Lee et al. \cite{lee2019sample}, by considering the last steps of the trajectories as the initial candidates for sampling in the current batch. Once all transitions from a trajectory are sampled, a new trajectory is included in the batch. A backward weight is applied to the Q-values of transitions within a trajectory to mitigate extrapolation errors in the calculation of TD error. Priorities are then assigned to entire trajectories rather than individual transitions, a method known as Prioritized Trajectory Replay (PTR). This approach explores various priority metrics based on the quality and degree of uncertainty of a trajectory, effectively addressing out-of-distribution problems and enhancing training, particularly in sparse reward environments. In contrast, our method, while not directly handling trajectories for learning, is capable of approximating the behavior of the implicit MDP governing a given environment, allowing us to overlook trajectories. However, our method is not constrained to single-step transitions and could potentially incorporate trajectories in future work.


% I'm here

\subsection{Priority Reweighing}

Finally, we analyze methods that aim to reweigh priorities more efficiently. Kumar et al. \cite{kumar2020discor}  propose a corrective feedback method, reweighing samples by adjusting the TD-error target value with an estimated target value error, mitigating error accumulation. Sun et al. \cite{sun2020attentive} propose Attention Experience Replay (AER), implicitly assigning high priorities to transitions containing frequently visited states. Liu et al. \cite{liu2021regret} propose an optimal prioritization strategy based on regret minimization, indicating transitions with higher hindsight TD error should be prioritized. This work implements two methods: ReMERN, learning an error network to assign priorities, and ReMERT, exploiting temporal state ordering without an additional neural network. Concurrently, SUNRISE \cite{lee2021sunrise} reweighs target Q-values based on uncertainty estimates from a Q-ensemble, improving signal-to-noise in Q-updates and stabilizing learning. Lastly, Sinha et al. \cite{sinha2022experience}  propose a reweighing technique, reassigning priorities by the likelihood-free density ratio between on-policy and off-policy experiences.


Density-based Prioritization: This method focuses on maintaining a diverse set of experiences in the replay buffer by ensuring that experiences are sampled from low-density regions of the state space. This helps in covering the state space more uniformly.
Likelihood-free Importance Weighting: Sinha et al. (2022) introduced a method that reweights priorities by estimating the likelihood-free density ratio between on-policy and off-policy experiences.


\subsection{Auxiliary Mechanism}
% \subsection{Model-based Sampling}

Additional methods focus on incorporating auxiliary mechanisms to reduce dependence on priority during sampling. Zha et al. \cite{zha2019experience} include an additional replay policy learned to filter irrelevant experiences and use priorities only on relevant ones. Similarly, Pan et al. \cite{pan2022understanding} propose a model-based Stochastic Gradient Langevin Dynamics (SGLD) sampling method, producing hypothetical experiences using the current policy and concatenating them with uniformly sampled experiences from an ER. SGLD addresses outdated priorities and insufficient sample space coverage. In contrast, by learning abstractions, our method does not require an additional replay policy learning, complex world model, or hypothetical state tracking. Instead, our method reduces training complexity by learning lower-dimensional state representations, which are organized by behavioral quality in a latent space.


Our work aligns with methods that reweigh priorities but also includes an auxiliary mechanism to calculate bisimulation metrics. Specifically, our method implicitly learns a bisimulation (and bisimulation metric) by learning state abstractions with an encoder neural network. This network is updated with an objective loss function, which encourages to keep behaviorally similar states closer together and behaviorally dissimilar states farther apart in a latent space. The bisimulation metric defines a behavioral similarity distance, which will be used to reweigh priorities, encouraging more diverse sampling.

\section{Bisimulation Metrics}

% An explanation of the overview of bisimulation metric, but the detailed understanding of what it needed to this thesis is in background, which was written before so mentioning this as other examples 

\subsection{Dynamical Programming Methods}
\subsection{Deep Learning Methods}
\subsection{Kernel Methods}
\section{Learning State Abstractions}
\subsection{Reconstructions Abstractions}
\subsection{Contrastive Abstractions}
\subsection{Behavioral Abstractions}
\subsubsection{Example}