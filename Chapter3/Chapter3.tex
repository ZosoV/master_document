% !TEX root =  ../Dissertation.tex

\chapter{Literature Review}

\section{Non-Uniform Sampling Experience Replays}

Prioritized Experience Replay (PER) \cite{schaul2015prioritized} is by far the most relevant non-uniform sampling method, which samples visited experiences proportional to the absolute TD errors, thereby efficiently reducing convergence time. Following these promising results, numerous attempts have been made to further improve and refine non-uniform sampling to address various challenges, such as sparse reward assignment \cite{andrychowicz2017hindsight, dai2021diversity}, experience retention \cite{de2018experience}, the bias-variance trade-off \cite{fedus2020revisiting, hessel2018rainbow, sutton1988learning, sutton2018reinforcement}, trajectory sampling \cite{dai2021diversity, liu2023prioritized}, among others. The wide variety of approaches can make it complex and unclear to differentiate between distinct and complementary methods. To clarify these distinctions, we categorize the methods into three groups that aim to capture the primary nuances of these methodologies: Priority Substitution, Priority Reweighting, and Auxiliary Mechanisms.

\subsection{Priority Substitution}

%Even though PER is considered a state-of-the-art method and is widely used in various reinforcement learning algorithms as a mechanism to alleviate long training loops, its efficiency declines in challenging scenarios, e.g: with sparse rewards, where the reward signal only appears after long trajectories.

Substitution-based non-uniform sampling methods replace the priority definition with an alternative proxy for expected learning progress, while still preserving the core concept of assigning a priority to each element in the experience replay.
 
Andrychowicz et al. \cite{andrychowicz2017hindsight} proposed a hindsight method using universal policies, known as Hindsight Experience Replay (HER). This method accepts both a current state and a goal state, encouraging the experience replay to sample experiences in hindsight; in other words, using a different goal on the fly than the one the agent was originally trying to achieve in the episode. HER effectively handles sparse and binary rewards without requiring additional reward manipulation. Subsequent efforts by de Bruim et al. \cite{de2018experience} explored a concept of experience selection, which address both retention and sampling by using different proxies to define priorities based on the immediate and long-term utility. They provided promising implementation guidelines to proxies that can be highly sensible to a task at hand. 

%Our method, in contrast, does not require defining different goals on the fly. Instead, by learning a bisimulation metric dynamically (considering both immediate rewards and transitions), it approximates the behavioral similarity of theoretically all states in the environment, thereby dealing with sparse rewards without the need for extra reward manipulation or goal-based policies.

% Subsequent efforts have studied the retention and sampling capabilities of experience replay simultaneously; focusing on how many experiences to maintain in and sample from the experience replay. 



%In contrast, our method is based on an strong mathematical formalism (bisimulation \cite{li2006towards}), applicable to any discrete or stochastic environment, and able to adjust to any task at hand, without the need for hand-engineering proxies.

Alternatively, Dopamine Rainbow \cite{hessel2018rainbow} empirically evaluates six alternatives of DQN algorithms, demonstrating the significant importance of prioritized replay and multi-step targets in enhancing DQN performance. Unlike single-step targets in temporal difference (TD) error, a multi-step target considers n steps with intermediate actions determined by the behavioral policy. This approach effectively substitutes single-step td-errors priorities for multi-step td-error priorities to train DQN algorithms. The multi-step bootstrap target \cite{sutton1988learning, sutton2018reinforcement} efficiently balances the bias-variance trade-off, enabling the rapid propagation of newly observed rewards to previously visited states. Fedus et al. \cite{fedus2020revisiting} extends this work by rigorously studying how different elements of ER impact the DQN algorithm. The findings reveal the imporatance of multi-step targets for leveraging replay buffers with large capacities, despite the significant degree of off-policyness they may introduce.

%Our method, BPER, assigns priority by weighting a single-step TD error and the bisimulation metric with a hyperparameter. Although our method could potentially overlook TD error in extreme cases where only the bisimulation metric is used for priority (i.e., when the maximum weight is set to 1), single-step targets are still necessary for optimizing the main reinforcement learning objective. Our approach does not impose any constraints on the use of n-step targets for both Q-learning and priority, and they can be incorporated to further enhance the performance of a DQN algorithm.

Recent works have explored the use of trajectories for both experience replay and priority assignment. Dai et al. \cite{dai2021diversity} proposed a two-stage method to enhance Hindsight Experience Replay (HER) by incorporating diversity-based trajectory sampling. First, trajectories are non-uniformly sampled from an experience replay buffer using priorities based on determinantal point processes (DPPs). Second, a k-DPP is applied to each trajectory to sample transitions with diverse goal states from the previously selected trajectories during training. Without relying on semantic knowledge of the goal space or tuning a curriculum, this method achieves efficient training and performance in robotic manipulation tasks. Subsequently, Liu et al. \cite{liu2023prioritized} further explored the concept of Trajectory Replay (TR), which stores complete trajectories instead of individual transition tuples. In these methods, transitions are sampled in a backward manner, building on the findings of Lee et al. \cite{lee2019sample}, by considering the last steps of the trajectories as the initial candidates for sampling in the current batch. Once all transitions from a trajectory are sampled, a new trajectory is included in the batch. Priorities are then assigned to entire trajectories rather than individual transitions, a method known as Prioritized Trajectory Replay (PTR). The method outperforms Atari baselines by achieving a fast and stable learning that only requires 10\% of samples.

%A backward weight is applied to the Q-values of transitions within a trajectory to mitigate extrapolation errors in the calculation of TD error.

%This approach explores various priority metrics based on the quality and degree of uncertainty of a trajectory, effectively addressing out-of-distribution problems and enhancing training, particularly in sparse reward environments. 

%In contrast, our method, while not directly handling trajectories for learning, is capable of approximating the behavior of the implicit MDP governing a given environment, allowing us to overlook trajectories. However, our method is not constrained to single-step transitions and could potentially incorporate trajectories in future work.



% I'm here

\subsection{Priority Reweighting}

Reweighting-based non-uniform sampling methods modify the priority definition by reweighting the TD-error in some manner, while still preserving the core concept of assigning a priority to each element in the experience replay.

Kumar et al. \cite{kumar2020discor} propose a corrective feedback method, reweighing samples by adjusting the TD-error target value with an estimated target value error, mitigating error accumulation. Their method, called DisCor, outperforms in various RL scenarios, such us Atari bechmark, maniputation tasks, and multi-task RL, and can easily be integrated in other valued-based algorithms. Consequently, Liu et al. \cite{liu2021regret} provided theoretical explanation for PER and DisCor methods, and proposed an optimal prioritization strategy based on regret minimization, indicating transitions with higher hindsight TD error should be prioritized. Their method ReMERT exploits temporal state ordering, showing outperforming results in RL benchmarks such as MuJoCo, Atari, and Meta-world. Concurrently, SUNRISE \cite{lee2021sunrise} reweighs target Q-values based on uncertainty estimates from a Q-ensemble, improving signal-to-noise in Q-updates and stabilizing learning. Their method was integrated in off-policy RL algorithms, such us SAC and Rainbow DQN, consistently, outperforming the state-of-the-art results. Lastly, Sinha et al. \cite{sinha2022experience} introduced a method that reweights priorities by the likelihood-free density ratio between on-policy and off-policy experiences. This method focuses on maintaining a diverse set of experiences in the replay buffer by ensuring that experiences are sampled from low-density regions of the state space. While their method is only applied to actor-critic methods, it shows promising results in 12 relevant Atari games, and 6 Deepmind control suite tasks.

% 12 Atari environments and 6 tasks from the DeepMind control suite. We achieve superior sample complexity on 9 out of 12 Atari environments and 16 out of 24 method-task combinations for DCS compared to the best baselines

% Likelihood-free Importance Weighting: Sinha et al. (2022) introduced a method that reweights priorities by estimating the likelihood-free density ratio between on-policy and off-policy experiences.

%Along side this work, Sun et al. \cite{sun2020attentive} propose Attentive Experience Replay (AER), implicitly assigning high priorities to transitions containing frequently visited states. 

\subsection{Auxiliary Mechanism}
% \subsection{Model-based Sampling}

Auxiliary mechanisms can be integrated with non-uniform sampling methods to modify the sampling procedure and reduce dependence on priority, without necessarily preserving to the core concept of assigning a priority to each element in the experience replay

Zha et al. \cite{zha2019experience} introduced an additional replay policy that learns to filter out irrelevant experiences and apply priorities only to the relevant ones. Their method was evaluated on various continuous control tasks using the DDPG algorithm, where it consistently demonstrated improved sample efficiency. Later, Liu et al. \cite{liu2021regret} proposed a method called ReMERN, which uses an error network to assign priorities. By training a task-specific neural network, this method ensures greater robustness across different environments, addressing various sources of randomness compared to the ReMERT alternative. Later, Pan et al. \cite{pan2022understanding} proposed a model-based Stochastic Gradient Langevin Dynamics (SGLD) sampling method, which generates hypothetical experiences using the current policy and combines them with uniformly sampled experiences from an ER. The proposed SGLD method efficiently addresses issues related to outdated priorities and insufficient sample space coverage and outperforms PER in classical environments. Finally, Zhao and Tresp \cite{zhao2019curiosity} introduced a Curiosity-driven Experience Prioritization mechanism, utilizing an auxiliary curiosity module to encourage the over-sampling of trajectories with rare achieved goal states, leading to a more balanced exploration and exploitation. This method can be integrated with any off-policy RL algorithm and was tested on six robot manipulation tasks, where it outperformed in terms of sample efficiency and final performance.

%In contrast, by learning abstractions, our method does not require an additional replay policy learning, complex world model, or hypothetical state tracking. Instead, our method reduces training complexity by learning lower-dimensional state representations, which are organized by behavioral quality in a latent space.

Our work aligns with methods that reweigh priorities but also includes an auxiliary mechanism to calculate bisimulation metrics. Specifically, our method learns to approximated a bisimulation metric by learning state abstractions with an encoder neural network. This network is updated with an objective loss function, which encourages to keep behaviorally similar states closer together and behaviorally dissimilar states farther apart in a latent space. The bisimulation metric defines a behavioral similarity distance, which will be used to reweigh priorities by trading off between single step TD-error and bisimulation distance, encouraging more diverse sampling.

% Additionally, our approach employs single-step targets and does not require hand-enginering proxies.

% does not impose any constraints on the use of n-step targets for both Q-learning and priority, and they can be incorporated to further enhance the performance of a DQN algorithm.
% hand-engineering proxies.

% \section{Bisimulation Metrics}

% % An explanation of the overview of bisimulation metric, but the detailed understanding of what it needed to this thesis is in background, which was written before so mentioning this as other examples 

% \subsection{Dynamical Programming Methods}
% \subsection{Deep Learning Methods}
% \subsection{Kernel Methods}
\section{Learning State Abstractions}

% In RL, there have been a lot of breakthrough successes including learning to play a diverse set of video games from raw pixels and outperform human performance \cite{mnih2013playing, mnih2015human}, continuous control tasks such as controlling a simulated car from a dashboard camera (Lillicrap et al., 2015) and subsequent algorithmic developments and applications to agents that successfully navigate mazes and solve complex tasks from first-person camera observations (Jaderberg et al., 2016; Espeholt et al., 2018; Jaderberg et al., 2019); and robots that successfully grasp objects in the real world (Kalashnikov et al., 2018).

RL is a well-studied topic in machine learning, with multiple success and diverse applications. Nonetheless, there are still certain challenges in practice that have been widely studied. Particularly, the \textbf{sample-inefficient problem}, which refers to the large amount of data (experiences) required to learn a feasible policy, is of interest for this current work.

According to works from Lake et al. \cite{lake2017building}, Kaiser et al. \cite{kaiser2019model}, and Laskin et al. \cite{laskin2020curl}, it has been empirically demonstrated that learning from high-dimensional observations such as raw pixel is sample-inefficient. In fact, more concise observations based on state-based features makes the learning of policies less computational intensive and sample-efficient than learning from pixels \cite{tassa2018deepmind}. According to Laskin et al. \cite{laskin2020curl}, it is feasible extract the relevant state information from pixel data, as long as this information is present in the raw data, making viable the learning of policies. Then, the only dilemma is to contextualize an adequate a method to extract (or learn) these representations. 

%state abstractions are proposed in order to learn more relevant representation from raw high dimensional observations (e.g. pixels), effectively dealing with the sample-inefficient problem.

Under these circumstances, in RL, a state abstraction is defined as a technique used to simplify the representation of the state space, making it more manageable and easier for the learning algorithm to process and understand. Specifically, a \textbf{state abstraction} \cite{abel2022theory} is a function, $\phi : S \rightarrow S_\phi$, that maps each true environmental state $s \in S$ into an abstract state $s_\phi \in S_\phi$, which lives in a given lower dimensional state space. This state abstraction is regularly an auxiliary task learned during training simultaneously with the main RL objective. In this work, we are interesting in learning a surrogate bisimulation metric, called MICO \cite{castro2021mico}, as part of the learning of state abstractions; thereby we identified three different approaches to learn these representations, termed as reconstruction-based, contrastive-based, and behavioral-based abstractions.

% This is particularly useful for real-world environments, which often have high-dimensional continuous state spaces, making them computationally challenging to handle directly. State abstraction helps to mitigate this issue by grouping similar states together or focusing on only the most relevant aspects of the state, effectively reducing the complexity of the state space.

\subsection{Reconstructions Abstractions}

Reconstruction-based abstractions use reconstruction objectives to reduce the dimensionality of the states, requiring additional hand-engineering to learn explicit temporal transition models in order to account for the dynamic properties of the RL environment. 

The most representative approach by far is World Models \cite{ha2018world}, which is a model-based RL method based on three models: a visual, a memory and a controller. In this approach, a variational autoencoder (visual) is trained together with a LSTM model that represents the memory, making possible to account for the environment transitions (temporal information). The low-dimensional temporal abstract representation in the visual model enables a direct training in the reduced state space without requiring to spent expensive training cycles in the actual environment. Along the same lines, Hafner et al. \cite{hafner2019learning} proposed a model-based agent, known as Deep Planning Network (PlaNet) that focus on planning in a learned latent space. Specifically, their proposal uses a multi-step variational inference objective (termed as Latent Overshooting), which aims to attempt two objectives (1) encoding a latent state and reconstruction of the original image, and (2) infer approximate posterior distributions of the latent states, which are consequently used to predict the next states and rewards. By learning this lower-dimensional latent space that captures the underlying state dynamic of the environment, the method achieves to solve several continuous control tasks using considerable less episodes compare to model-free algorithms.

% a variational autoencoders are used with two objectives: (1) reconstruct the image observations i




 % In the visual module, an autoencoder is leveraged as an auxiliary task to reduce the dimensionality of input raw-pixel frames from a simulator and learn an abstract, compressed representation. Consequently, the visual is trained together with a LSTM model that represents the memory, which is able to account for the environment transitions (temporal information).
 
 %Finally, a small controller is trained to guide the next actions.

% Reconstruction-based representations are particularly useful when it is essential to prioritize the structural (visual) information of the states. For example: ensemble methods using different cameras at the same time, which together return a more informative reconstructed representation of the state [CITE].

% However, World Models\cite{ha2018world} are by far the most representative and complete approach.

While reconstruction-based methods is a viable solution to learn state abstractions, they may encode parts of the observations that are not relevant to a given task. After all, the inherit unsupervised learning that governs a variational auto-encoder cannot, by definition, know what will be useful for the RL task at hand. 

%It focuses on capturing information to be able to reconstruct the original image, but not all the features needed to reconstruct the image are necessarily useful for the task at hand.

\subsection{Contrastive Abstractions}

Contrastive-based abstractions focus on learning more useful semantic representation by distinguishing between similar (positive) and dissimilar (negative) pairs of data points, typically by maximizing the distance between negatives and minimizing the distance between positives in a latent space. %Intuitively, the algorithm will group together latent representations of similar entities (positive pairs) and move far away latent representations of different entities (negative pairs).

In this context, van den Oord et al. \cite{oord2018representation} proposed Contrastive Predictive Coding (CPC), an unsupervised learning approach for representation learning, which uses autoregressive models (e.g. LSTM) for predicting future data points in a latent space. To do that, the method uses Noise-Contrastive Estimation (NCE) to contrast true future samples and a set of randomly sampled negative examples. Their method provides promising performance on several domains, suggesting a possible universal method for different modalities. Consequently, in the RL domain, Srinivas et al. \cite{laskin2020curl} eliminates the dependency on autoregresive pipelines by incorporating implicit temporal information through augment temporal sequential frames. Their method, CURL, includes a simple instance auxiliary contrastive learning model that maximizes agreement between augmented versions of the same state, where each state is a stack of temporally sequential frames. The positive and negative pairs are chosen, performing instance discrimination on those frame stacks, which allow to learn both spatial and temporal discriminative features. Along the same lines, Anand et al. \cite{anand2019unsupervised} proposed a contrastive approach based on maximizing mutual information between local, global, and temporal features. To do that, an InfoNCE technique is used where the positive samples corresponds to temporarily close states (e.g. the state in the next time step), and negative samples corresponds to distant temporal states. This approach successfully learns more granular semantic representations, such as agent location and objects, while ignoring irrelevant information, such as background textures.


% TODO: Add dreamer paper if I need more

% Contrastive-based representations are especially valuable when it's crucial to discern subtle but significant differences between similar-looking states. For instance, in environments like robot navigation within visually uniform settings (e.g., similar corridors or rooms), contrastive learning helps differentiate essential navigational cues that dictate distinct actions, improving decision-making precision in tasks where such nuances are critical.

Contrastive-based abstractions learn efficient semantic state representations without relying on reconstruction losses. However, they still have certain limitations because they do not incorporate information about the downstream task that guides the RL system. According to Zhang et al. \cite{zhang2020learning}, these methods are task-agnostic.

% While contrastive losses do not require reconstruction, they do not inherently have a mechanism to determine the relevance of the downstream task without manual engineering, and when trained only for prediction, they aim to capture all predictable features in the observation, which performs poorly on real images for the same reasons reconstruction-based representations do.

\subsection{Behavioral Abstractions}

Behavioral-based abstractions incorporate the downstream task knowledge, by considering the behavioral equivalence between states. Specifically, bisimulation metrics serve as a form of state abstraction that groups states \(s_i\) and \(s_j\) that are 'behaviorally equivalent'. Roughly speaking, we can understand that two states are behavioural equivalent if they can get similar long-term expected accumulated reward (the RL objective). %Roughly speaking, the bisimulation metric is an indicator of how behavioral similar are two states, where high values correspond to behavioral dissimilarity and low values to high behavioral similarity.  

Deep bisimulation for control (DBC) \cite{zhang2020learning} leverages this metric to learn task-aware invariant representations by assuming that the model that governs the transitions in the implicit MDP follow a Gaussian distribution. In this approach, an encoder is trained to minimize the mean square error between the on-policy bisimulation metric and L1 distance in the latent space. By distributing the states according to the bisimulation metric in the latent space, this method achieves to account for behavioral similarity, avoiding distractors in images (e.g: clouds for an automatic driving system). Additionally, as the bisimulation metric considers explicitly transitions and rewards, it achieves to address the temporal dynamics of the system when learning representations. Consequently, Castro et al. \cite{castro2021mico} successfully eliminates the Gaussian assumptions by proposing a surrogate on-policy bisimulation metric, which work more generally in deterministic and stochastic environment, and with different value-based algorithms. Their method, Matching under Independent Coupling (MICo), replace the computational expensive calculation of the Kantorovich term for the independent coupling, providing a higher theoretical bound, but still capturing behavioral similarity. As a result, the incorporation of MICo learning on other algorithms allows to outperform them in Atari Learning Environments and DM-Control benchmarks. Finally, Castro et al. \cite{castro2023kernel} proposes an alternative perspective of MICo metric by using positive definite kernels. They introduced a state similarity kernel in the state space, which induces the reduced MICo distance \cite{castro2021mico}, yielding similar strong results, and providing a more complete theoretical analysis.


%Intuitively, the algorithm will group together behavioral similar latent representations, and move far away behavioral dissimilar latent representations \footnote{Notice that behavioral similarity doesn’t correspond necessarily to visual (structural) similarity}.

%Behavioral-based representations are particularly useful when it is essential to prioritize the relevant information avoiding distractors. For example: in an autonomous driving system where the images can include objects like tree, house, sky and sun; where distractors like the sky and sun are not relevant to learn the task of automatic driving.


% \footnote{This is particular constrasting with other methods that learns state representation with reconstruction losses, which require adding extra parameter to attain for these reconstructions capabilities}.
% Here we don't have extract parameters to learn the representation (extrat parameters for reconstruction losses), but the issue is that when the agent needs to make decision those extra parameters are not used, because those extra parameters are not used for the control part.