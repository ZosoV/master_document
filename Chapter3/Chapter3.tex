% !TEX root =  ../Dissertation.tex

\chapter{Literature Review}

\section{Non-Uniform Sampling Experience Replays}

Prioritized Experience Replay (PER) \cite{schaul2015prioritized} is by far the most relevant non-uniform sampling method, which samples visited experiences proportional to the absolute TD errors, thereby efficiently reducing convergence time. However, following these promising results, numerous attempts have been made to further improve and refine non-uniform sampling to address various challenges, such as sparse reward assignment \cite{andrychowicz2017hindsight, dai2021diversity}, experience retention \cite{de2018experience}, the bias-variance trade-off \cite{fedus2020revisiting, hessel2018rainbow, sutton1988learning, sutton2018reinforcement}, and trajectory sampling \cite{dai2021diversity, liu2023prioritized}. The wide variety of approaches can make it complex and unclear to differentiate between distinct and complementary methods. To clarify this landscape, the following proposed categorization aims to capture the main nuances of these methodologies and provide guidelines for exploring the current experience replay methods in the literature. The categorization is as follows: Priority Substitution, Priority Reweighting, and Auxiliary Learnable Mechanisms.

\subsection{Priority Substitution}

 Even though PER is considered a state-of-the-art method and is widely used in various reinforcement learning algorithms as a mechanism to alleviate long training loops, its efficiency declines in challenging scenarios, e.g: with sparse rewards, where the reward signal only appears after long trajectories. To address this issue, Andrychowicz et al. \cite{andrychowicz2017hindsight} proposed a hindsight method using universal policies, known as \textbf{Hindsight Experience Replay (HER)}. This method accepts both a current state and a goal state, encouraging the experience replay to sample experiences in hindsight; in other words, using a different goal on the fly than the one the agent was originally trying to achieve in the episode. HER effectively handles sparse and binary rewards without requiring additional reward manipulation. Our method, in contrast, does not require defining different goals on the fly. Instead, by learning a bisimulation metric dynamically (considering both immediate rewards and transitions), it approximates the behavioral similarity of theoretically all states in the environment, thereby dealing with sparse rewards without the need for extra reward manipulation or goal-based policies.

Subsequent efforts have studied the retention and sampling capabilities of experience replay simultaneously; focusing on how many experiences to maintain in and sample from the experience replay.  Particularly, de Bruim et al. \cite{de2018experience} has explored a concept of \textbf{experience selection}, which address both retention and sampling by using different proxies to define priorities based on the immediate and long-term utility. However, they only provided implementation guidelines because the proxies can be highly sensible to the task at hand. In contrast, our method is based on an strong mathematical formalism (bisimulation \cite{li2006towards}), applicable to any discrete or stochastic environment, and able to adjust to any task at hand, without the need for hand-engineering proxies.

Alternatively, Dopamine Rainbow \cite{hessel2018rainbow} empirically evaluates six alternatives of DQN algorithms, demonstrating the significant importance of prioritized replay and \textbf{multi-step targets} in enhancing DQN performance. Unlike single-step targets in temporal difference (TD) error, a multi-step target considers n steps with intermediate actions determined by the behavioral policy. This approach effectively substitutes single-step td-errors priorities for multi-step td-error priorities to train DQN algorithms. The multi-step bootstrap target \cite{sutton1988learning, sutton2018reinforcement} efficiently balances the bias-variance trade-off, enabling the rapid propagation of newly observed rewards to previously visited states. Consequently, Fedus et al. \cite{fedus2020revisiting} rigorously studied the effect of various components of experience replay on the DQN algorithm, revealing that multi-step targets are crucial for leveraging replay buffers with large capacities, despite the significant degree of off-policyness they may introduce. Our method, BPER, assigns priority by weighting a single-step TD error and the bisimulation metric with a hyperparameter. Although our method could potentially overlook TD error in extreme cases where only the bisimulation metric is used for priority (i.e., when the maximum weight is set to 1), single-step targets are still necessary for optimizing the main reinforcement learning objective. Our approach does not impose any constraints on the use of n-step targets for both Q-learning and priority, and they can be incorporated to further enhance the performance of a DQN algorithm.

Recent works have explored the use of \textbf{trajectories} for both experience replay and priority assignment. Dai et al. \cite{dai2021diversity} proposed a two-stage method to enhance Hindsight Experience Replay (HER) by incorporating diversity-based trajectory sampling. First, trajectories are non-uniformly sampled from an experience replay buffer using priorities based on determinantal point processes (DPPs). Second, a k-DPP is applied to each trajectory to sample transitions with diverse goal states from the previously selected trajectories during training. Without relying on semantic knowledge of the goal space or tuning a curriculum, this method achieves efficient training and performance in robotic manipulation tasks. Subsequently, Liu et al. \cite{liu2023prioritized} further explored the concept of Trajectory Replay (TR), which stores complete trajectories instead of individual transition tuples. In these methods, transitions are sampled in a backward manner, building on the findings of Lee et al. \cite{lee2019sample}, by considering the last steps of the trajectories as the initial candidates for sampling in the current batch. Once all transitions from a trajectory are sampled, a new trajectory is included in the batch. A backward weight is applied to the Q-values of transitions within a trajectory to mitigate extrapolation errors in the calculation of TD error. Priorities are then assigned to entire trajectories rather than individual transitions, a method known as Prioritized Trajectory Replay (PTR). This approach explores various priority metrics based on the quality and degree of uncertainty of a trajectory, effectively addressing out-of-distribution problems and enhancing training, particularly in sparse reward environments. In contrast, our method, while not directly handling trajectories for learning, is capable of approximating the behavior of the implicit MDP governing a given environment, allowing us to overlook trajectories. However, our method is not constrained to single-step transitions and could potentially incorporate trajectories in future work.


% I'm here

\subsection{Priority Reweighing}

Finally, we analyze methods that aim to reweigh priorities more efficiently. Kumar et al. \cite{kumar2020discor}  propose a corrective feedback method, reweighing samples by adjusting the TD-error target value with an estimated target value error, mitigating error accumulation. Sun et al. \cite{sun2020attentive} propose Attention Experience Replay (AER), implicitly assigning high priorities to transitions containing frequently visited states. Liu et al. \cite{liu2021regret} propose an optimal prioritization strategy based on regret minimization, indicating transitions with higher hindsight TD error should be prioritized. This work implements two methods: ReMERN, learning an error network to assign priorities, and ReMERT, exploiting temporal state ordering without an additional neural network. Concurrently, SUNRISE \cite{lee2021sunrise} reweighs target Q-values based on uncertainty estimates from a Q-ensemble, improving signal-to-noise in Q-updates and stabilizing learning. Lastly, Sinha et al. \cite{sinha2022experience}  propose a reweighing technique, reassigning priorities by the likelihood-free density ratio between on-policy and off-policy experiences.


Density-based Prioritization: This method focuses on maintaining a diverse set of experiences in the replay buffer by ensuring that experiences are sampled from low-density regions of the state space. This helps in covering the state space more uniformly.
Likelihood-free Importance Weighting: Sinha et al. (2022) introduced a method that reweights priorities by estimating the likelihood-free density ratio between on-policy and off-policy experiences.


\subsection{Auxiliary Mechanism}
% \subsection{Model-based Sampling}

Additional methods focus on incorporating auxiliary mechanisms to reduce dependence on priority during sampling. Zha et al. \cite{zha2019experience} include an additional replay policy learned to filter irrelevant experiences and use priorities only on relevant ones. Similarly, Pan et al. \cite{pan2022understanding} propose a model-based Stochastic Gradient Langevin Dynamics (SGLD) sampling method, producing hypothetical experiences using the current policy and concatenating them with uniformly sampled experiences from an ER. SGLD addresses outdated priorities and insufficient sample space coverage. In contrast, by learning abstractions, our method does not require an additional replay policy learning, complex world model, or hypothetical state tracking. Instead, our method reduces training complexity by learning lower-dimensional state representations, which are organized by behavioral quality in a latent space.


Our work aligns with methods that reweigh priorities but also includes an auxiliary mechanism to calculate bisimulation metrics. Specifically, our method implicitly learns a bisimulation (and bisimulation metric) by learning state abstractions with an encoder neural network. This network is updated with an objective loss function, which encourages to keep behaviorally similar states closer together and behaviorally dissimilar states farther apart in a latent space. The bisimulation metric defines a behavioral similarity distance, which will be used to reweigh priorities, encouraging more diverse sampling.

% \section{Bisimulation Metrics}

% % An explanation of the overview of bisimulation metric, but the detailed understanding of what it needed to this thesis is in background, which was written before so mentioning this as other examples 

% \subsection{Dynamical Programming Methods}
% \subsection{Deep Learning Methods}
% \subsection{Kernel Methods}
\section{Learning State Abstractions}

In this context, there have been a lot of breakthrough successes including learning to play a diverse set of video games from raw pixels and outperform human performance (Mnih et al., 2015), continuous control tasks such as controlling a simulated car from a dashboard camera (Lillicrap et al., 2015) and subsequent algorithmic developments and applications to agents that successfully navigate mazes and solve complex tasks from first-person camera observations (Jaderberg et al., 2016; Espeholt et al., 2018; Jaderberg et al., 2019); and robots that successfully grasp objects in the real world (Kalashnikov et al., 2018).

Nonetheless, it has been empirically observed that reinforcement learning from high dimensional observations such as raw pixels is sample-inefficient (Lake et al., 2017; Kaiser et al., 2019). In view of this problem, state abstractions are proposed in order to learn more relevant representation of the states in a MDP, effectively dealing with high dimensional observation. In RL, a state abstraction is a technique used to simplify the representation of the state space in a problem, making it more manageable and easier for the learning algorithm to process and understand.

An state abstraction is a function, φ ∶ S → Sφ, that maps each true environmental state s ∈ S into an abstract state sφ ∈ Sφ, which lives in a given lower dimensional state space. This state abstraction is regularly an auxiliary task learned during training simultaneously with the main RL objective. 

For example: an state abstraction could lead us to learn a compress representation of similar states such as 


This is particularly useful for real-world environments, which often have high-dimensional continuous state spaces, making them computationally challenging to handle directly. State abstraction helps to mitigate this issue by grouping similar states together or focusing on only the most relevant aspects of the state, effectively reducing the complexity of the state space.

2. Applications: Three Approaches

In this review, I identify three different applications to learn these representations, termed as reconstruction, contrastive and behavioral abstractions.

Reconstruction-based abstractions use reconstruction objectives to reduce the dimensionality of the states, requiring additional hand-engineering to learn explicit temporal transition models in order to account for the dynamic properties of the RL environment. The most representative approach by far is World Models based on three models: visual, memory and controller. In the visual module, an autoencoder is leveraged as an auxiliary task to reduce the dimensionality of input raw-pixel image from a simulator and learn an abstract, compressed representation of each observed input frame. Consequently, the visual is trained together with a LSTM model that represents the memory, which is able to account for the environment transitions (temporal information). Finally, a small controller is trained to guide the next actions. The low-dimensional temporal abstract representation produced by the autoencoder allows us to avoid wasting training cycles in the actual environment, but instead train the agent as many times as we want in a reduced state space. Reconstruction-based representations are particularly useful when it is essential to prioritize the structural (visual) information of the states. For example: ensemble methods using different cameras at the same time, which together return a more informative reconstructed representation of the state [CITE].

However, the usage of reconstruction-based methods for learning abstract representations has limitations, since they may encode parts of the observations that are not relevant to a task. After all, this unsupervised learning cannot, by definition, know what will be useful for the RL task at hand. It only focuses on capturing information to be able to reconstruct the original image, but not all the features needed to reconstruct the image are necessarily useful for the task at hand.

In consequence, the contrastive-based abstractions focus on learning more useful semantic representation by distinguishing between similar (positive) and dissimilar (negative) pairs of data points, typically by maximizing the distance between negatives and minimizing the distance between positives in a feature space. Intuitively, in the end, the algorithm will group together latent representations of similar entities (positive pairs) and move far away latent representations of different entities (negative pairs). The most representative example is CURL, which includes an auxiliary contrastive learning model that maximizes agreement between augmented versions of the same state, where each state is a stack of temporally sequential frames. The temporal nature of the transition is addressed implicitly by this stack of temporal sequential frames. The positive and negative pairs are chosen, performing instance discrimination on those frame stacks, which allow to learn both spatial and temporal discriminative features, attempting in some degree the temporal dynamics of the problem. Contrastive-based representations are especially valuable when it's crucial to discern subtle but significant differences between similar-looking states. For instance, in environments like robot navigation within visually uniform settings (e.g., similar corridors or rooms), contrastive learning helps differentiate essential navigational cues that dictate distinct actions, improving decision-making precision in tasks where such nuances are critical.

While contrastive losses do not require reconstruction, they do not inherently have a mechanism to determine the relevance of the downstream task without manual engineering, and when trained only for prediction, they aim to capture all predictable features in the observation, which performs poorly on real images for the same reasons reconstruction-based representations do. A better method would be to incorporate knowledge of the downstream task into the similarity function in a data-driven way, so that images that are very different pixel-wise (e.g. lighting or texture changes), can also be grouped as similar w.r.t. downstream objectives. 

Behavioral-based abstractions incorporate these downstreams task knowledge, by considering the behavioral equivalence between states. Specifically, bisimulation metrics serve as a form of state abstraction that groups states \(s_i\) and \(s_j\) that are 'behaviorally equivalent'. We can understand that two states are behavioural equivalent if they can get similar long-term expected accumulated reward (the RL objective). Roughly speaking, the bisimulation metric is an indicator of how behavioral similar are two states, where high values correspond to behavioral dissimilarity and low values to high behavioral similarity.  Deep bisimulation for control (DBC) is a good example of how to leverage this metric to learn task-aware invariant representations. It is still trained with two encoders but looks to minimize the mean square error between the on-policy bisimulation metric and L1 distance in the latent space. Intuitively, in the end, the algorithm will group together behavioral similar latent representations, and move far away behavioral dissimilar latent representations. Notice that behavioral similarity doesn’t correspond necessarily to visual (structural) similarity, as we can see in the following example. By distributing the states according to the bisimulation metric in the latent space, this method achieves to account for behavioral similarity, avoiding distractor in images (e.g: low-intensity images could be considered the same behaviorally). Additionally, as the bisimulation metric considers explicitly transitions and rewards, it achieves to address the temporal dynamic of the system when learning representations. Behavioral-based representations are particularly useful when it is essential to prioritize the relevant information avoiding distractors. For example: in an autonomous driving system where the images can include objects like tree, house, sky and sun; where distractors like the sky and sun are not relevant to learn the task of automatic driving.

\subsection{Reconstructions Abstractions}
\subsection{Contrastive Abstractions}
\subsection{Behavioral Abstractions}
\subsubsection{Example}