@article{mnih2013playing,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013}
}

@article{lillicrap2015continuous,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015}
}

@inproceedings{haarnoja2018soft,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={1861--1870},
  year={2018},
  organization={PMLR}
}

@article{schaul2015prioritized,
  title={Prioritized experience replay},
  author={Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  journal={arXiv preprint arXiv:1511.05952},
  year={2015}
}

@inproceedings{castro2020scalable,
  title={Scalable methods for computing state similarity in deterministic markov decision processes},
  author={Castro, Pablo Samuel},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={06},
  pages={10069--10076},
  year={2020}
}

@article{li2006towards,
  title={Towards a unified theory of state abstraction for MDPs.},
  author={Li, Lihong and Walsh, Thomas J and Littman, Michael L},
  journal={AI\&M},
  volume={1},
  number={2},
  pages={3},
  year={2006},
  publisher={Citeseer}
}

@article{givan2003equivalence,
  title={Equivalence notions and model minimization in Markov decision processes},
  author={Givan, Robert and Dean, Thomas and Greig, Matthew},
  journal={Artificial Intelligence},
  volume={147},
  number={1-2},
  pages={163--223},
  year={2003},
  publisher={Elsevier}
}

@article{ferns2011bisimulation,
  title={Bisimulation metrics for continuous Markov decision processes},
  author={Ferns, Norm and Panangaden, Prakash and Precup, Doina},
  journal={SIAM Journal on Computing},
  volume={40},
  number={6},
  pages={1662--1714},
  year={2011},
  publisher={SIAM}
}

@inproceedings{ferns2014bisimulation,
  title={Bisimulation Metrics are Optimal Value Functions.},
  author={Ferns, Norman and Precup, Doina},
  booktitle={UAI},
  pages={210--219},
  year={2014}
}

@inproceedings{ferns2004metrics,
  title={Metrics for Finite Markov Decision Processes.},
  author={Ferns, Norm and Panangaden, Prakash and Precup, Doina},
  booktitle={UAI},
  volume={4},
  pages={162--169},
  year={2004}
}

@article{zhang2020learning,
  title={Learning invariant representations for reinforcement learning without reconstruction},
  author={Zhang, Amy and McAllister, Rowan and Calandra, Roberto and Gal, Yarin and Levine, Sergey},
  journal={arXiv preprint arXiv:2006.10742},
  year={2020}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@inproceedings{pan2022understanding,
  title={Understanding and mitigating the limitations of prioritized experience replay},
  author={Pan, Yangchen and Mei, Jincheng and Farahmand, Amir-massoud and White, Martha and Yao, Hengshuai and Rohani, Mohsen and Luo, Jun},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={1561--1571},
  year={2022},
  organization={PMLR}
}

@inproceedings{fedus2020revisiting,
  title={Revisiting fundamentals of experience replay},
  author={Fedus, William and Ramachandran, Prajit and Agarwal, Rishabh and Bengio, Yoshua and Larochelle, Hugo and Rowland, Mark and Dabney, Will},
  booktitle={International Conference on Machine Learning},
  pages={3061--3071},
  year={2020},
  organization={PMLR}
}

@article{chen2023attention,
  title={Attention Loss Adjusted Prioritized Experience Replay},
  author={Chen, Zhuoying and Li, Huiping and Wang, Rizhong},
  journal={arXiv preprint arXiv:2309.06684},
  year={2023}
}

@article{castro2021mico,
  title={Mico: Improved representations via sampling-based state similarity for markov decision processes},
  author={Castro, Pablo Samuel and Kastner, Tyler and Panangaden, Prakash and Rowland, Mark},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={30113--30126},
  year={2021}
}

@article{abel2022theory,
  title={A theory of abstraction in reinforcement learning},
  author={Abel, David},
  journal={arXiv preprint arXiv:2203.00397},
  year={2022}
}

@article{andrychowicz2017hindsight,
  title={Hindsight experience replay},
  author={Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Pieter Abbeel, OpenAI and Zaremba, Wojciech},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{de2018experience,
  title={Experience selection in deep reinforcement learning for control},
  author={De Bruin, Tim and Kober, Jens and Tuyls, Karl and Babu{\v{s}}ka, Robert},
  journal={Journal of Machine Learning Research},
  volume={19},
  number={9},
  pages={1--56},
  year={2018}
}

@article{zha2019experience,
  title={Experience replay optimization},
  author={Zha, Daochen and Lai, Kwei-Herng and Zhou, Kaixiong and Hu, Xia},
  journal={arXiv preprint arXiv:1906.08387},
  year={2019}
}

@article{kumar2020discor,
  title={Discor: Corrective feedback in reinforcement learning via distribution correction},
  author={Kumar, Aviral and Gupta, Abhishek and Levine, Sergey},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={18560--18572},
  year={2020}
}

@inproceedings{sun2020attentive,
  title={Attentive experience replay},
  author={Sun, Peiquan and Zhou, Wengang and Li, Houqiang},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={04},
  pages={5900--5907},
  year={2020}
}

@article{liu2021regret,
  title={Regret minimization experience replay in off-policy reinforcement learning},
  author={Liu, Xu-Hui and Xue, Zhenghai and Pang, Jingcheng and Jiang, Shengyi and Xu, Feng and Yu, Yang},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17604--17615},
  year={2021}
}

@inproceedings{lee2021sunrise,
  title={Sunrise: A simple unified framework for ensemble learning in deep reinforcement learning},
  author={Lee, Kimin and Laskin, Michael and Srinivas, Aravind and Abbeel, Pieter},
  booktitle={International Conference on Machine Learning},
  pages={6131--6141},
  year={2021},
  organization={PMLR}
}

@inproceedings{sinha2022experience,
  title={Experience replay with likelihood-free importance weights},
  author={Sinha, Samarth and Song, Jiaming and Garg, Animesh and Ermon, Stefano},
  booktitle={Learning for Dynamics and Control Conference},
  pages={110--123},
  year={2022},
  organization={PMLR}
}

@article{abate2024bisimulation,
  title={Bisimulation learning},
  author={Abate, Alessandro and Giacobbe, Mirco and Schnitzer, Yannik},
  journal={arXiv preprint arXiv:2405.15723},
  year={2024}
}

@book{baier2008principles,
  title={Principles of model checking},
  author={Baier, Christel and Katoen, Joost-Pieter},
  year={2008},
  publisher={MIT press}
}

@inproceedings{hessel2018rainbow,
  title={Rainbow: Combining improvements in deep reinforcement learning},
  author={Hessel, Matteo and Modayil, Joseph and Van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32},
  number={1},
  year={2018}
}

@article{sutton1988learning,
  title={Learning to predict by the methods of temporal differences},
  author={Sutton, Richard S},
  journal={Machine learning},
  volume={3},
  pages={9--44},
  year={1988},
  publisher={Springer}
}

@article{liu2023prioritized,
  title={Prioritized Trajectory Replay: A Replay Memory for Data-driven Reinforcement Learning},
  author={Liu, Jinyi and Ma, Yi and Hao, Jianye and Hu, Yujing and Zheng, Yan and Lv, Tangjie and Fan, Changjie},
  journal={arXiv preprint arXiv:2306.15503},
  year={2023}
}

@article{lee2019sample,
  title={Sample-efficient deep reinforcement learning via episodic backward update},
  author={Lee, Su Young and Sungik, Choi and Chung, Sae-Young},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{dai2021diversity,
  title={Diversity-based trajectory and goal selection with hindsight experience replay},
  author={Dai, Tianhong and Liu, Hengyan and Arulkumaran, Kai and Ren, Guangyu and Bharath, Anil Anthony},
  booktitle={PRICAI 2021: Trends in Artificial Intelligence: 18th Pacific Rim International Conference on Artificial Intelligence, PRICAI 2021, Hanoi, Vietnam, November 8--12, 2021, Proceedings, Part III 18},
  pages={32--45},
  year={2021},
  organization={Springer}
}

@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group UK London}
}