% !TEX root =  ../Dissertation.tex

\chapter{Conclusion and Future Work}

Even though it returns promising results, one thing that I realized is that the bisimulation values are relative too high compare with the td-errors, which can produce to focus too much in some experiences and unven and drastically skewness probability distribution, a way to soft the sampling distribution could be recommended. 

We have tried to normalize using a logaritmic scale and max min, but it didn't worked properly.

Even when using the 0.01 beta in the loss function

Theoretical proof

It lacks of a theoretical basis

also some experimetns suggest that most of the improvements comes from the MICO and not actually the prioritization

a sweep over the alpha and beta values, although we use the best one for PER, they could not be the best ones for BPER

Additionally, a way to normalize the priorities or another sampling method like the rank based because the BPER priorities are larger than the td-error (ojo).

Try Double DQN and other algorithms and better benchmarks.

Check with more state of the art algorithms and methods


% VISA

% For visits of up to 6 months for most purposes.
% Cost: CAN \$100

% https://neurips.cc/Conferences/2024/Dates

% https://www.canada.ca/en/immigration-refugees-citizenship/services/visit-canada/about-visitor-visa.html

% https://www.canada.ca/content/dam/ircc/documents/pdf/english/kits/forms/imm5645/01-01-2021/imm5645e.pdf

% https://www.canada.ca/en/immigration-refugees-citizenship/services/visit-canada/apply-visitor-visa.html