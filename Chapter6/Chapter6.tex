% !TEX root =  ../Dissertation.tex

\chapter{Conclusion and Future Work}
\label{sec:conclusion_future_work}

\section{Conclusion}
In this work, we proposed a non-uniform sampling technique to sample transitions from an experience replay, named Bisimulation Prioritized Experience Replay (BPER) along with two strategies: current-vs-next (BPERcn) and all-vs-all (BPERaa). The priorities are assigned based on a surrogate on-policy bisimulation, called MICo metric, that is used to encourage sampling transitions with more behavioral dissimilar states (promoting data diversity). The MICo metric is estimated as part of the learning of state abstractions. The proposed bisimulation-based prioritization technique shows promising results, demonstrating strong performance in a GridWorld environment used to validate the effectiveness of the method.

In the GridWorld experiments, the results indicate that the BPERcn and BPERaa strategies effectively address three key sampling challenges: task-agnostic sampling, outdated priorities, and state space coverage. Both visual and quantitative analyses show that our approach outperforms all other evaluated methods (DQN, DQN + PER, and DQN + MICO) in each of these areas, demonstrating the viability of our method.

In slightly more complex environment, however, the proposed approach is partially effective, consistently outperforming the direct baseline DQN + MICo, but outperforming DQN and DQN + PER only in special cases.

The method offers a potentially valuable approach for future applications, but its successful implementation will require careful consideration of certain factors. These include the structure of the environment, such as the reward function, the action space, and the transitions. Further research is needed to explore these aspects in depth and to refine the method for broader applicability.

\section{Future Work}

The following are future directions for the current work:

\begin{itemize}
    \item \textbf{Complex Environment to Evaluate.} The MICO operator, as defined in Definition \ref{def:mico_operator}, utilizes both the reward difference and the independent coupling calculated over transitions. In such a way, the impact of MICO learning (inherently influenced by the structure of the environment) becomes more evident in complex environments with a greater variety of actions and rewards. In this project, we tested the method only on relatively simple environments with limited actions and rewards, which made it complicated to fully discern its effectiveness. We recommend to explore more complex environments in future works.
    \item \textbf{Heavily Skewed Sampling Distributions.} The bisimulation values are significantly higher compared to the TD-errors (see Figure \ref{fig:log_mean_batch_priority_methods}). Although high priority values in a minibatch can be associated with positive effects, consistently assigning excessively large values is not ideal, as it may lead to uneven and heavily skewed probability distributions that disproportionately weight certain experiences over others. A potential solution to mitigate this issue could involve using normalization techniques\footnote{We conducted experiments using a logarithmic and min-max scaling not yielding satisfactory results.} or clipping techniques before assigning priorities. Additionally, alternative sampling methods, such as temperature-based sampling (commonly used in large language models \cite{jm3}) or rank-based sampling \cite{schaul2015prioritized}, could be explored.
    \item \textbf{Lack of Theoretical Guarantees.} This work provides an empirical evaluation of the proposed method; however, it does not include a theoretical proof that guarantees the algorithm's convergence. Future work should explore developing a formal proof to establish such guarantees. We suggest following approaches similar to those in Pan et al. \cite{pan2022understanding}, which provided a formal proof for the convergence of PER.
    \item \textbf{Prioritization Hyper-parameter Adjustments.} In the evaluated experiments, the best alpha $\alpha$ and beta $\beta$ hyperparameters, which control the prioritization level ($\alpha$-probability) and the weighted importance sampling for the PER \cite{schaul2015prioritized} method, were used. However, although these hyperparameters are optimal for the PER method, they may not necessarily be the best for our approach. We recommend to conduct a hyperparameter sweep to find the adequate setting for our method.
    \item \textbf{Benchmarks and Algorithms.} Similar to MICO learning \cite{castro2021mico},
    our method can be integrated into any value-based agent, adapted to different algorithms that  employs an experience replay as part of the learning, such as Double DQN \cite{van2016deep}, DDPG \cite{lillicrap2015continuous}, and SAC \cite{haarnoja2018soft}. Additionally, in this work, we have only tested our approach in classical environments without considering other relevant benchmarks, such as Atari \cite{bellemare2013arcade, machado2018revisiting} or MuJoCo \cite{todorov2012mujoco} bechmarks. Future work should explore the performance of our method with these state-of-the-art algorithms and benchmarks.
    \item \textbf{Annealing Priority Weighting.} Since some experiments have shown that TD-error can serve as an effective prioritization method earlier during training,it would be worthwhile to explore annealing the priority weighting. This approach would begin with a weighting that favors TD-error and gradually shift towards bisimulation-based prioritization as the algorithm improves its approximation of the MICO metric over time.
\end{itemize}

% VISA

% For visits of up to 6 months for most purposes.
% Cost: CAN \$100

% https://neurips.cc/Conferences/2024/Dates

% https://www.canada.ca/en/immigration-refugees-citizenship/services/visit-canada/about-visitor-visa.html

% https://www.canada.ca/content/dam/ircc/documents/pdf/english/kits/forms/imm5645/01-01-2021/imm5645e.pdf

% https://www.canada.ca/en/immigration-refugees-citizenship/services/visit-canada/apply-visitor-visa.html