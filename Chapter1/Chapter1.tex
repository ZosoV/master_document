% !TEX root =  ../Dissertation.tex

\chapter{Introduction}
\label{sec:introduction}

Incorporating deep learning techniques into Reinforcement Learning (RL) frameworks has been challenging due disparity in data assumptions between deep learning and RL algorithms \cite{mnih2013playing}. Traditional deep learning relies on the independence of data samples for effective neural network training, whereas RL is characterized by a temporal sequential process that results in \textbf{highly correlated states}. Moreover, the data distribution in RL is \textbf{non-stationary}; it evolves as the algorithm acquires new behaviors. This dynamism leads to instability when deep learning techniques are applied to RL algorithms. 
%, which typically assumes a fixed and identical underlying distribution.

% Incorporating deep learning techniques into Reinforcement Learning (RL) frameworks has been challenging due disparity in data assumptions between deep learning and RL algorithms \cite{mnih2013playing}. Traditional deep learning relies on the independence of data samples for effective neural network training, whereas RL is characterized by a temporal sequential process that results in \textbf{highly correlated states}. Moreover, deep learning requires an identical data distribution, but in RL is these data distribution are \textbf{non-stationary}; it evolves as the algorithm acquires new behaviors. This dynamism leads to instability when deep learning are applied to RL algorithms. 

Experience Replay (ER) has been implemented in online RL algorithms, such as DQN \cite{mnih2013playing}, DDPG \cite{lillicrap2015continuous}, SAC \cite{haarnoja2018soft} to address both data correlation and non-stationary distributions issues. Essentially, an ER functions as a database where experience tuples\footnote[2]{An experience tuple consists of (state $s_{t}$, action $a_{t}$, reward $R_{t}$, next state $s_{t+1}$)} are stored, allowing training to occur on minibatches sampled from this experience buffer. This simple mechanism facilitates breaking temporal data correlations, leading to approximate independent and identically distributed (iid) data distributions.

While ER benefits online RL, significant iterations may still be required for convergence. Schaul et al. \cite{schaul2015prioritized} note that a DQN algorithm revisits the same experience tuple an average of eight times, not all of which lead to significant improvements. In consequence, they proposed a Prioritized Experience Replay (PER), assigning probabilities to each experience based on the Temporal Difference (TD) error \cite{sutton2018reinforcement}. The TD-error priority works as an indicator of the \textbf{Expected Learning Progress (ELP)} \cite{schaul2015prioritized}\footnote{According to Schaul et al. \cite{schaul2015prioritized}, the \textbf{Expected Learning Progress} (ELP) is the idealised criterion to assign a priority value with the amount the RL agent can learn from a transition in its current state.}; encouraging more frequently replay experiences which lead to higher improvements. However, this prioritization can face several issues, such as task-agnostic sampling, outdated priorities, and insufficient state space coverage.

Prioritizing purely on TD-error can overlook the task-specific behaviors of states, leading to what we term as \textbf{task-agnostic sampling} problem, similar to representation learning findings in \cite{zhang2020learning}. From this perspective, PER fails to recognize that certain states in a Markov Decision Process (MDP), despite being structural dissimilar, can exhibit similar long-term behaviors (w.r.t. the RL downstream task), resulting in similar expected returns in the long run. In other words, it fails to recognize that some states can be behavioral similar. The \textbf{outdated priorities} limitation, in the other hand, arises from practical implementation issues. According to Theorem 1 in Pan Y. et al. \cite{pan2022understanding}, priorities should be updated over all experiences in the replay buffer using updated training parameters at each time step in order to obtain a faster convergence rate. However, this approach is impractical due to the large replay buffer capacity, which imposes a high computational cost when updating all priorities during each learning iteration. PER \cite{schaul2015prioritized} methods update only the priorities of experiences from a sampled mini-batch, leaving others unchanged, resulting in inaccurate sampling distributions. Indeed, PER is quite restrictive because priorities are calculated only from experience tuples already visited and stored in the experience buffer, representing just a small subset of the entire state space. This exploration issue, called \textbf{insufficient sample space coverage}, has been highlighted by other sources \cite{fedus2020revisiting,pan2022understanding}. Lack of sample space coverage can lead to data imbalance issues, when during learning the experiences are unevenly distributed in the state space \cite{chen2023attention}, producing high correlated transitions due to high levels of correlation between the data-generating distribution (the experience replay) and the current evaluated policy \cite{fedus2020revisiting}, tending to an on-policy updating behavior.

Bisimulation metrics \cite{ferns2004metrics, ferns2011bisimulation, ferns2014bisimulation, castro2020scalable} provides a means of quantifying the behavioral similarity between states by considering the immediate rewards along with how states transition under a given MDP. Leveraging this behavioral concept aims to prioritize more informative tuples in the experience replay by identifying state pairs with significant behavioral differences, as they often correspond to more \textit{'surprising'} transitions, leading to more effective improvements. Using bisimulation metrics also alleviates outdated priorities by providing more realistic long-term behavior definitions. Although bisimulation-based priorities will be still updated in the sampled mini-batch, the behavioral-based bisimulation metrics theoretically rely on a dynamic programming operator (commonly used in RL, such as the value iteration algorithm \cite{sutton2018reinforcement}), which is proven to converge to a fixed point where the distance metric remains constant \cite{castro2020scalable, castro2021mico}. Unlike TD-error, this provides a more realistic and stable definition of priority in the long term, alleviating the outdated priorities. Additionally, prioritizing behavioral dissimilar states encourages data diversity and exploration, tackling the insufficient sample space coverage problem. Specifically, learning state abstractions with a bisimulation metric organizes latent codes in a latent space based on behavior, inducing large state space coverage of behavioral dissimilar states. 
% In this work, we will explore the on-policy bisimulation metric proposed by Castro \cite{castro2020scalable}, which specifically emphasizes the dynamics induced by the current policy. Specifically, a surrogate on-policy bisimulation metric called Matching under Independent Couplings (MICO) \cite{castro2021mico} metric, which works more efficiently under both discrete and stochastic environments, will be obtained as part of the learning of state abstractions. According to \cite{zhang2020learning} and \cite{castro2021mico}, the calculation of a bisimulation metric (or surrogate) in an online manner as part of the learning of state abstractions is a more effective way to obtain this metric.

\section{Contributions}

In this work, we propose incorporate a bisimulation metric as priority in a prioritized experience replay by learning state abstractions. We explored the on-policy bisimulation metric proposed by Castro \cite{castro2020scalable}, which specifically emphasizes the dynamics of the Markov Chain induced by the current policy. Specifically, we used a surrogate on-policy bisimulation metric called Matching under Independent Couplings (MICO) \cite{castro2021mico} metric, which works more efficiently under both discrete and stochastic environments and is obtained as part of the learning of state abstractions. According to \cite{zhang2020learning} and \cite{castro2021mico}, the calculation of a bisimulation metric (or surrogate) in an online manner as part of the learning of state abstractions is a more effective way to obtain this metric. Our Bisimulation Prioritized Experience Replay (BPER) aims to 1) emphasize behaviorally relevant transitions, thereby avoiding task-agnostic experience sampling, 2) alleviate the outdated priorities by having a better tendency to constant fixed priorities, and 3) mitigate the insufficient sample space coverage by encouraging the sampling of behavioral dissimilar states, increasing the data diversity.

\section{Document Structure}

% The following thesis document is organized in several chapters, which should be read in principle sequential. We understand that there are two eseential background information to understand this thesis, which are the RL and Bisimulation foundations. Specifically, we put a lot of emphasis that the reader should read first the background section in order to profoundly understand the methodology.

% However, it is still possible to skip some sections for certain use cases.

% \begin{itemize}
%     \item For the reader familiarized with reinforcement learning, the Section \ref{} can be skipped.
%     \item For a fast track understanding of the methodology, we recommend to explore the introduction section \ref{} and methodology section, skipping the motivating example, which requires a lot of connection with the background section.
%     \item If the reader is looking to possible ways to extend this work. We just recommend to explore the introduction and directly check conclusion to understand the state of the work.
% \end{itemize}

This thesis is organized into several chapters that should generally be read sequentially. We recognize that there are two essential areas of background knowledge required to fully understand this work: the foundations of RL and Bisimulation. We strongly encourage readers to first read the background (Section \ref{sec:background}) to gain a comprehensive understanding of the methodology. However, certain sections may be skipped depending on the reader's familiarity and specific interests:

\begin{itemize} 
    \item Readers already familiar with reinforcement learning may choose to skip Section \ref{sec:reinforcement_learning}. 
    \item For a quick understanding of the methodology, we suggest reviewing the Introduction (Section \ref{sec:introduction}) and the Bisimulation Prioritized Experience Replay (Section \ref{sec:bper_method}), while skipping the Motivating Example (Section \ref{sec:motivating_example}), which relies heavily on the background material.
    \item For those interested in potential extensions of this work, we recommend focusing on the Introduction (Section \ref{sec:introduction}) and then proceeding directly to the Conclusion and Future Work (Section \ref{sec:conclusion_future_work}) to grasp the current state of the research. 
\end{itemize}



%, Non-uniform Sampling Methods, and Learning State Abstractions. In such way, we divided these content in background and literature review, by giving more weight to the background because it's essential to understand the following chapters.

\section{Legal, Social, Ethical and Professional Issues}

\begin{itemize}
    \item \textbf{Legal.} The proposed method utilizes data that is openly accessible from reinforcement learning environments available in \href{https://gymnasium.farama.org/}{Gymnasium}. For the development of the custom Grid World, we used and modified the open-source code provided by the \href{https://github.com/Farama-Foundation/gym-examples/blob/main/gym_examples/envs/grid_world.py}{Farama-Foundation/gym-examples}. To replicate the MICo learning approach, we retrieved the openly available code from \href{https://github.com/google-research/google-research/tree/master/mico}{Google Research}. All datasets in this repository are released under the CC BY 4.0 International license, which can be reviewed \href{https://creativecommons.org/licenses/by/4.0/legalcode}{here}, while the source files are licensed under the Apache 2.0 license. For the reproduction of the DQN algorithm, we employed the state-of-the-art implementation provided by the \href{https://github.com/pytorch/rl/tree/main/sota-implementations/dqn}{TorchRL repository}, licensed under the MIT License. All necessary acknowledgments and citations have been duly provided.
    \item \textbf{Social.} The present work, in its current research state, does not produce any social bias. However, if applied to real-world scenarios, such as autonomous vehicles or healthcare decision systems, there is a need to ensure that the algorithms do not reinforce existing biases or result in unintended consequences that could adversely affect certain social groups. 
    \item \textbf{Ethical.} The present work, in its current research state, does not produce any ethical bias. However, if applied to real-world scenarios, such as surveillance systems, there is a need to continually assess whether these approaches unintentionally disadvantage certain outcomes or populations.
    \item \textbf{Professional Issues.} The present work follows guidelines suggested in professional and academic sectors, such as the British Computer Society (BCS) Code of Conduct, the ACM and IEEE. This includes maintaining integrity, competence, and responsibility in the development and implementation of RL methods.  
\end{itemize}

