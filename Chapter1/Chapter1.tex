% !TEX root =  ../Dissertation.tex

\chapter{Introduction}

Incorporating deep learning techniques into Reinforcement Learning (RL) frameworks has been challenging due disparity in data assumption between deep learning and RL schemes \cite{mnih2013playing}. Traditional deep learning relies on the independence of data samples for effective neural network training, whereas RL is characterized by a temporal sequential process that results in \textbf{highly correlated states}. Moreover, the data distribution in RL is \textbf{non-stationary}; it evolves as the algorithm acquires new behaviors. This dynamism leads to instability in deep learning, which typically assumes a fixed and identical underlying distribution.

Experience Replay (ER) has been implemented in online RL algorithms, such as DQN \cite{mnih2013playing}, DDPG \cite{lillicrap2015continuous}, SAC \cite{haarnoja2018soft} to address both data correlation and non-stationary distributions issues. It facilitates breaking temporal data correlations, leading to approximate independent and identically distributed (iid) data distributions.

While ER benefits online RL, significant iterations may still be required for convergence. Schaul et al. \cite{schaul2015prioritized} note that a DQN algorithm revisits the same experience tuple\footnote[2]{An experience tuple consists of (state $s_{t}$, action $a_{t}$, reward $R_{t}$, next state $s_{t+1}$)} an average of eight times, not all of which lead to significant improvements. In consequence, they proposed a Prioritized Experience Replay (PER), assigning probabilities to each experience based on the Temporal Difference (TD) error \cite{sutton2018reinforcement}. The TD-error priority works as an indicator of the \textbf{expected learning progress}; encouraging more frequently replay experiences which lead to higher improvements. However, this prioritization can face several issues, such as task-agnostic sampling, outdated priorities, and insufficient state space coverage.

Prioritizing purely on TD-error can overlook the task-specific behaviors of states, leading to what we term as \textbf{task-agnostic sampling} problem, similar to representation learning findings in \cite{zhang2020learning}. This perspective fails to recognize that certain states in a Markov Decision Process (MDP), despite being structural dissimilar, can exhibit similar long-term behaviors under the same policy, resulting in similar expected returns in the long run. The \textbf{outdated priorities} limitation, in the other hand, arises from practical implementation issues. According to Theorem 1 in Pan Y. et al. \cite{pan2022understanding}, priorities should be updated over all experiences in the replay buffer using updated training parameters at each time step in order to obtain a faster convergence rate. However, this approach is impractical due to the large replay buffer capacity, which imposes a high computational cost when updating all priorities during each learning iteration. PER \cite{schaul2015prioritized} methods update only the priorities of experiences from a sampled mini-batch, leaving others unchanged, resulting in inaccurate priority distributions. Indeed, PER is quite restrictive because priorities are calculated only from experience tuples already visited and stored in the experience buffer, representing just a small subset of the entire state space. This exploration issue, called \textbf{insufficient sample space coverage}, has been highlighted by other sources \cite{fedus2020revisiting,pan2022understanding}. Lack of sample space coverage can lead to data imbalance issues, when during learning the experiences are unevenly distributed in the state space \cite{chen2023attention}, producing high correlated transitions due to high levels of correlation between the data-generating distribution (the experience replay) and the current evaluated policy \cite{fedus2020revisiting}, tending to an on-policy updating behavior.

Bisimulation metrics \cite{ferns2004metrics, ferns2011bisimulation, ferns2014bisimulation, castro2020scalable} provides a means of quantifying the behavioral similarity between states by considering both the immediate rewards and the expected future rewards (discounted over time), along with how states transition under a given policy. Leveraging this behavioral concept aims to prioritize more informative tuples in the experience replay by identifying state pairs with significant behavioral differences, as they often correspond to more \textit{'surprising'} transitions, leading to more effective improvements. Additionally, prioritizing behavioral dissimilar states encourages data diversity and exploration, tackling the insufficient sample space coverage problem. Specifically, learning state abstractions with a bisimulation metric organizes latent codes in a latent space based on behavior, inducing large state space coverage of behavioral dissimilar states. Finally, using bisimulation metrics also alleviates outdated priorities, providing more realistic long-term behavior definitions. Although priorities are still updated in the sampled mini-batch, the behavioral-based bisimulation metrics theoretically rely on a dynamic programming operator (commonly used in RL, such as the value iteration algorithm \cite{sutton2018reinforcement}), which is proven to converge to a fixed point where the distance metric remains constant \cite{castro2020scalable, castro2021mico}. Unlike TD-error, this provides a more realistic and stable definition of priority in the long term.

In this work, we will explore the on-policy bisimulation metric proposed by Castro \cite{castro2020scalable}, which specifically emphasizes the dynamics induced by the current policy. Specifically, a surrogate on-policy bisimulation metric called Matching under Independent Couplings (MICO) \cite{castro2021mico} metric, which works more efficiently under both discrete and stochastic environments, will be obtained as part of the learning of state abstractions. According to \cite{zhang2020learning} and \cite{castro2021mico}, the calculation of a bisimulation metric (or surrogate) in an online manner as part of the learning of state abstractions is a more effective way to obtain this metric.

\section{Contributions}

In this work, we propose learning state abstractions to incorporate a bisimulation metric as priority in a prioritized experience replay. We explored the on-policy bisimulation metric proposed by Castro \cite{castro2020scalable}, which specifically emphasizes the dynamics induced by the current policy. Specifically, we used a surrogate on-policy bisimulation metric called Matching under Independent Couplings (MICO) \cite{castro2021mico} metric, which works more efficiently under both discrete and stochastic environments and is obtained as part of the learning of state abstractions. As noted in \cite{zhang2020learning} and \cite{castro2021mico}, calculating a bisimulation metric (or its surrogate) online during state abstraction learning is a more effective approach. Our Bisimulation Prioritized Experience Replay (BPER) aims to 1) emphasize behaviorally relevant transitions, thereby avoiding task-agnostic experience sampling, 2) alleviate the outdated priorities by having a better tendency to constant fixed priorities, and 
3) mitigate the insufficient sample space coverage by encouraging the sampling of behavioral dissimilar states, increasing the data diversity.

\section{Document Structure}

For the reader don't familiarized with reinforcement learning, we recommend start by looking to the RL background informaiton on. Otherwise, we recommend to start by Bisimulation Background.

Literature review helps us to position our method in the state of the art, but it could be skipped if the objective is to understand the method. I will just recommend to read a little bit about state abstractions, which is a fundamental part for the proposed method in this thesis.

After that we recommend explore methodology and result and discussion.


NOTE: I think I need to included something about how the thesis is structured.
This thesis documents is structured in such a way 

